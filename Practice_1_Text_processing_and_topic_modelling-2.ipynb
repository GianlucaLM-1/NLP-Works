{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practice_1_Text_processing_and_topic_modelling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZIl1U-b7AzU"
      },
      "source": [
        "# **Deep Natural Language Processing @ PoliTO**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Teaching Assistant:** Moreno La Quatra\n",
        "\n",
        "**Practice 1:** Text processing and topic modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1vQPzjM9r8c"
      },
      "source": [
        "# **Text processing**\n",
        "---\n",
        "The text processing phase is a preliminary stage where the text to be manipulated is processed to be ready for subsequent analysis.\n",
        "\n",
        "Text processing usually entails several steps that could possibly include:\n",
        "- **Language Identification**: identifying the language of a given text.\n",
        "- **Tokenization**: splitting a given text in several sentences/words. \n",
        "- **Dependency tree parsing:** analyzing the depencies between words composing the text.\n",
        "- **Stemming/Lemmatization:** obtain the root form for each word in text.\n",
        "- **Stopword removal**: removing words that are si commonly used that they carry very little useful information.\n",
        "- **Part of Speech Tagging:** given a word, retrieve its part of speech (proper noun, common noun or verb).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2dHRmrPB22r"
      },
      "source": [
        "### Language Identification\n",
        "\n",
        "| Text                                                                                                                                | Language Code |\n",
        "|-------------------------------------------------------------------------------------------------------------------------------------|---------------|\n",
        "| The \"Deep Natural Language Processing\" course is offered during the first semester of the second year at Politecnico di Torino      | `EN`            |\n",
        "| Il corso \"Deep Natural Language Processing\" viene impartito al Politecnico di Torino durante il primo semestre del secondo anno.    | `IT`            |\n",
        "| Le cours \"Deep Natural Language Processing\" est enseigné au Politecnico di Torino pendant le premier semestre de la deuxième année. | `FR`            |\n",
        "\n",
        "**Language Identification** is a crucial prelimiary step because each language has its own characteristics. The knowledge of the main language associated to a given text could be beneficial for all subsequent steps in text processing pipeline.\n",
        "\n",
        "The data collection used in this first part of the practice is provided [here](https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P1/langid_dataset.csv) - [source: Kaggle](https://www.kaggle.com/martinkk5575/language-detection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ej_dfjm2srd"
      },
      "source": [
        "# Exercise 1:\n",
        "\n",
        "Benchmark different language-detection algorithm by computing the accuracy of each approach:\n",
        "- [FastText](https://pypi.org/project/fastlangid/)\n",
        "- [LangID](https://github.com/saffsd/langid.py)\n",
        "- [langdetect](https://pypi.org/project/langdetect/)\n",
        "\n",
        "**Hint:** language code conversion: [iso639-lang](https://pypi.org/project/iso639-lang/)\n",
        "\n",
        "For each method report:\n",
        "- Accuracy\n",
        "- Average time per example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ7xbkps3_mR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7eec3cc-fc32-4fcf-b594-b07d87a87fb2"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P1/langid_dataset.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-06 15:29:53--  https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P1/langid_dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12990065 (12M) [text/plain]\n",
            "Saving to: ‘langid_dataset.csv.3’\n",
            "\n",
            "\rlangid_dataset.csv.   0%[                    ]       0  --.-KB/s               \rlangid_dataset.csv. 100%[===================>]  12.39M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2021-10-06 15:29:53 (139 MB/s) - ‘langid_dataset.csv.3’ saved [12990065/12990065]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-509xNcU0tcU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9db4a66a-fa1b-4094-d543-2b4ae8d6dfba"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"langid_dataset.csv\")\n",
        "\n",
        "y = df['language']\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>klement gottwaldi surnukeha palsameeriti ning ...</td>\n",
              "      <td>Estonian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
              "      <td>Swedish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
              "      <td>Thai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
              "      <td>Tamil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
              "      <td>Dutch</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  language\n",
              "0  klement gottwaldi surnukeha palsameeriti ning ...  Estonian\n",
              "1  sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
              "2  ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
              "3  விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
              "4  de spons behoort tot het geslacht haliclona en...     Dutch"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2Za4Nq5wfn6"
      },
      "source": [
        "# FastText\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-ssrJ_Gu-RZ",
        "outputId": "6182bbfa-8b48-412a-9225-8bf5daf9f6ee"
      },
      "source": [
        "%%time\n",
        "from fastlangid.langid import LID\n",
        "\n",
        "langid = LID()\n",
        "\n",
        "results_fasttext = langid.predict(df['Text'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.48 s, sys: 32.1 ms, total: 3.52 s\n",
            "Wall time: 3.74 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amAqU2-lwmnY"
      },
      "source": [
        "# LangId"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY7XCd0Gwmwm",
        "outputId": "d1e824f2-7b18-46d4-e41d-e353c4fdd8e3"
      },
      "source": [
        "%%time\n",
        "import langid\n",
        "\n",
        "results_langid = []\n",
        "for i in range(len(df)):\n",
        "    det = langid.classify(df['Text'][i])\n",
        "    results_langid.append(det[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min, sys: 1min 5s, total: 2min 5s\n",
            "Wall time: 1min 11s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb55bBvvwm4d"
      },
      "source": [
        "LangDetect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwZpJKr11X3_"
      },
      "source": [
        "from langdetect import detect\n",
        "\n",
        "for i in range(len(df['Text'])):\n",
        "    try:\n",
        "        language = detect(df['Text'][i])\n",
        "    except:\n",
        "        df.drop([i])\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcXcKBafwm_b",
        "outputId": "8f8d0200-bb6d-4ca0-dc94-d8ba743a7ccd"
      },
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "results_langdetect = []\n",
        "for elem in df['Text']:\n",
        "    results_langdetect.append(detect(elem))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('et', -883.7894897460938)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7msUTRq_xWas"
      },
      "source": [
        "# Results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lkd9zez50VL"
      },
      "source": [
        "from iso639 import Lang\n",
        "res1 = []\n",
        "for element in results_fasttext:\n",
        "    if element.startswith(\"zh\"):\n",
        "        element = \"zh\"\n",
        "    lg = Lang(element)\n",
        "    res1.append(lg.name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xflUsnIA7uPr",
        "outputId": "de45335a-3814-4010-c784-8eea6965b134"
      },
      "source": [
        "from iso639 import Lang\n",
        "res2 = []\n",
        "for element in results_langid:\n",
        "    if element.startswith(\"zh\"):\n",
        "        element = \"zh\"\n",
        "    lg = Lang(element)\n",
        "    res2.append(lg.name)\n",
        "res2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Estonian',\n",
              " 'English',\n",
              " 'Thai',\n",
              " 'Tamil',\n",
              " 'Dutch',\n",
              " 'Japanese',\n",
              " 'Turkish',\n",
              " 'Latin',\n",
              " 'Urdu',\n",
              " 'Japanese',\n",
              " 'Indonesian',\n",
              " 'Portuguese',\n",
              " 'French',\n",
              " 'Chinese',\n",
              " 'Korean',\n",
              " 'Thai',\n",
              " 'Estonian',\n",
              " 'Portuguese',\n",
              " 'English',\n",
              " 'Hindi',\n",
              " 'Tamil',\n",
              " 'Spanish',\n",
              " 'French',\n",
              " 'French',\n",
              " 'Estonian',\n",
              " 'Korean',\n",
              " 'French',\n",
              " 'Pushto',\n",
              " 'Dutch',\n",
              " 'Persian',\n",
              " 'French',\n",
              " 'Romanian',\n",
              " 'Russian',\n",
              " 'Japanese',\n",
              " 'Indonesian',\n",
              " 'Latin',\n",
              " 'Latin',\n",
              " 'English',\n",
              " 'French',\n",
              " 'Portuguese',\n",
              " 'English',\n",
              " 'Urdu',\n",
              " 'English',\n",
              " 'Indonesian',\n",
              " 'Indonesian',\n",
              " 'Japanese',\n",
              " 'Arabic',\n",
              " 'Pushto',\n",
              " 'Indonesian',\n",
              " 'Swedish',\n",
              " 'Dutch',\n",
              " 'Russian',\n",
              " 'Dutch',\n",
              " 'Arabic',\n",
              " 'Arabic',\n",
              " 'Turkish',\n",
              " 'Urdu',\n",
              " 'French',\n",
              " 'Portuguese',\n",
              " 'Portuguese',\n",
              " 'Indonesian',\n",
              " 'Indonesian',\n",
              " 'Tamil',\n",
              " 'Swedish',\n",
              " 'Persian',\n",
              " 'Russian',\n",
              " 'English',\n",
              " 'Korean',\n",
              " 'Arabic',\n",
              " 'Thai',\n",
              " 'Portuguese',\n",
              " 'Arabic',\n",
              " 'Tamil',\n",
              " 'Pushto',\n",
              " 'Urdu',\n",
              " 'Pushto',\n",
              " 'English',\n",
              " 'Russian',\n",
              " 'Persian',\n",
              " 'Japanese',\n",
              " 'Portuguese',\n",
              " 'Hindi',\n",
              " 'Persian',\n",
              " 'English',\n",
              " 'Swedish',\n",
              " 'Indonesian',\n",
              " 'Latin',\n",
              " 'French',\n",
              " 'Latin',\n",
              " 'Pushto',\n",
              " 'English',\n",
              " 'Estonian',\n",
              " 'Malay (macrolanguage)',\n",
              " 'Tamil',\n",
              " 'Tamil',\n",
              " 'Latin',\n",
              " 'Latin',\n",
              " 'English',\n",
              " 'Pushto',\n",
              " 'Korean',\n",
              " 'Japanese',\n",
              " 'Russian',\n",
              " 'Tamil',\n",
              " 'Persian',\n",
              " 'Thai',\n",
              " 'Turkish',\n",
              " 'Estonian',\n",
              " 'English',\n",
              " 'Swedish',\n",
              " 'Japanese',\n",
              " 'Chinese',\n",
              " 'Thai',\n",
              " 'French',\n",
              " 'Arabic',\n",
              " 'Malay (macrolanguage)',\n",
              " 'Spanish',\n",
              " 'Arabic',\n",
              " 'Korean',\n",
              " 'Thai',\n",
              " 'Arabic',\n",
              " 'Estonian',\n",
              " 'Persian',\n",
              " 'Chinese',\n",
              " 'Japanese',\n",
              " 'Turkish',\n",
              " 'Estonian',\n",
              " 'Russian',\n",
              " 'English',\n",
              " 'Thai',\n",
              " 'Urdu',\n",
              " 'Russian',\n",
              " 'Russian',\n",
              " 'Hindi',\n",
              " 'Korean',\n",
              " 'Persian',\n",
              " 'Swedish',\n",
              " 'Urdu',\n",
              " 'Russian',\n",
              " 'Latin',\n",
              " 'Latin',\n",
              " 'Korean',\n",
              " 'Tamil',\n",
              " 'Turkish',\n",
              " 'Swedish',\n",
              " 'Hindi',\n",
              " 'Hindi',\n",
              " 'Turkish',\n",
              " 'Hindi',\n",
              " 'Thai',\n",
              " 'Hindi',\n",
              " 'Turkish',\n",
              " 'Chinese',\n",
              " 'Portuguese',\n",
              " 'Turkish',\n",
              " 'Urdu',\n",
              " 'English',\n",
              " 'English',\n",
              " 'French',\n",
              " 'English',\n",
              " 'Thai',\n",
              " 'Korean',\n",
              " 'Japanese',\n",
              " 'Spanish',\n",
              " 'English',\n",
              " 'Turkish',\n",
              " 'Portuguese',\n",
              " 'English',\n",
              " 'English',\n",
              " 'French',\n",
              " 'Thai',\n",
              " 'Japanese',\n",
              " 'Tamil',\n",
              " 'Korean',\n",
              " 'Dutch',\n",
              " 'Swedish',\n",
              " 'Persian',\n",
              " 'Indonesian',\n",
              " 'Dutch',\n",
              " 'Estonian',\n",
              " 'Turkish',\n",
              " 'Portuguese',\n",
              " 'Estonian',\n",
              " 'Arabic',\n",
              " 'Turkish',\n",
              " 'French',\n",
              " 'Pushto',\n",
              " 'Urdu',\n",
              " 'Thai',\n",
              " 'Romanian',\n",
              " 'Russian',\n",
              " 'French',\n",
              " 'Spanish',\n",
              " 'Portuguese',\n",
              " 'Persian',\n",
              " 'Persian',\n",
              " 'Spanish',\n",
              " 'Romanian',\n",
              " 'Japanese',\n",
              " 'Thai',\n",
              " 'Turkish',\n",
              " 'Turkish',\n",
              " 'Russian',\n",
              " 'Swedish',\n",
              " 'Swedish',\n",
              " 'Portuguese',\n",
              " 'Dutch',\n",
              " 'Indonesian',\n",
              " 'Hindi',\n",
              " 'Dutch',\n",
              " 'Swedish',\n",
              " 'Pushto',\n",
              " 'Estonian',\n",
              " 'Malay (macrolanguage)',\n",
              " 'Korean',\n",
              " 'Japanese',\n",
              " 'Pushto',\n",
              " 'Italian',\n",
              " 'Turkish',\n",
              " 'Portuguese',\n",
              " 'Persian',\n",
              " 'Pushto',\n",
              " 'Hindi',\n",
              " 'Latin',\n",
              " 'Dutch',\n",
              " 'Hindi',\n",
              " 'Russian',\n",
              " 'Russian',\n",
              " 'Chinese',\n",
              " 'Romanian',\n",
              " 'Arabic',\n",
              " 'English',\n",
              " 'Latin',\n",
              " 'Indonesian',\n",
              " 'Russian',\n",
              " 'Latin',\n",
              " 'English',\n",
              " 'Swedish',\n",
              " 'English',\n",
              " 'Pushto',\n",
              " 'Tamil',\n",
              " 'Hindi',\n",
              " 'Estonian',\n",
              " 'Latin',\n",
              " 'Russian',\n",
              " 'Indonesian',\n",
              " 'Arabic',\n",
              " 'English',\n",
              " 'Hindi',\n",
              " 'Pushto',\n",
              " 'Urdu',\n",
              " 'Chinese',\n",
              " 'Swedish',\n",
              " 'Dutch',\n",
              " 'English',\n",
              " 'Persian',\n",
              " 'Turkish',\n",
              " 'Japanese',\n",
              " 'Pushto',\n",
              " 'Pushto',\n",
              " 'Indonesian',\n",
              " 'Korean',\n",
              " 'Portuguese',\n",
              " 'Spanish',\n",
              " 'Urdu',\n",
              " 'English',\n",
              " 'Russian',\n",
              " 'Thai',\n",
              " 'Chinese',\n",
              " 'Persian',\n",
              " 'Turkish',\n",
              " 'Arabic',\n",
              " 'Urdu',\n",
              " 'Latin',\n",
              " 'Estonian',\n",
              " 'Urdu',\n",
              " 'Persian',\n",
              " 'Chinese',\n",
              " 'Latin',\n",
              " 'Thai',\n",
              " 'Thai',\n",
              " 'English',\n",
              " 'Romanian',\n",
              " 'Estonian',\n",
              " 'Afrikaans',\n",
              " 'Swedish',\n",
              " 'Spanish',\n",
              " 'Russian',\n",
              " 'Spanish',\n",
              " 'Russian',\n",
              " 'Korean',\n",
              " 'Arabic',\n",
              " 'Estonian',\n",
              " 'Turkish',\n",
              " 'Russian',\n",
              " 'Urdu',\n",
              " 'Estonian',\n",
              " 'Russian',\n",
              " 'Portuguese',\n",
              " 'Estonian',\n",
              " 'Portuguese',\n",
              " 'Japanese',\n",
              " 'Malay (macrolanguage)',\n",
              " 'Portuguese',\n",
              " 'Urdu',\n",
              " 'Portuguese',\n",
              " 'Hindi',\n",
              " 'Estonian',\n",
              " 'Hindi',\n",
              " 'Korean',\n",
              " 'Hindi',\n",
              " 'Russian',\n",
              " 'Chinese',\n",
              " 'Thai',\n",
              " 'Swedish',\n",
              " 'Hindi',\n",
              " 'Japanese',\n",
              " 'English',\n",
              " 'Spanish',\n",
              " 'Hindi',\n",
              " 'Russian',\n",
              " 'Latin',\n",
              " 'Romanian',\n",
              " 'Turkish',\n",
              " 'Japanese',\n",
              " 'Thai',\n",
              " 'Estonian',\n",
              " 'Indonesian',\n",
              " 'Persian',\n",
              " 'Spanish',\n",
              " 'Persian',\n",
              " 'Urdu',\n",
              " 'Tamil',\n",
              " 'Persian',\n",
              " 'French',\n",
              " 'Russian',\n",
              " 'Korean',\n",
              " 'Arabic',\n",
              " 'Turkish',\n",
              " 'French',\n",
              " 'English',\n",
              " 'French',\n",
              " 'Swedish',\n",
              " 'Portuguese',\n",
              " 'Persian',\n",
              " 'Swedish',\n",
              " 'Pushto',\n",
              " 'Latin',\n",
              " 'Persian',\n",
              " 'English',\n",
              " 'Japanese',\n",
              " 'Turkish',\n",
              " 'Urdu',\n",
              " 'Latin',\n",
              " 'Persian',\n",
              " 'Russian',\n",
              " 'English',\n",
              " 'Portuguese',\n",
              " 'Arabic',\n",
              " 'Romanian',\n",
              " 'Japanese',\n",
              " 'Swedish',\n",
              " 'Romanian',\n",
              " 'Indonesian',\n",
              " 'Persian',\n",
              " 'Pushto',\n",
              " 'Estonian',\n",
              " 'Russian',\n",
              " 'Chinese',\n",
              " 'Dutch',\n",
              " 'Dutch',\n",
              " 'Portuguese',\n",
              " 'Portuguese',\n",
              " 'Urdu',\n",
              " 'French',\n",
              " 'Russian',\n",
              " 'Urdu',\n",
              " 'Dutch',\n",
              " 'English',\n",
              " 'Japanese',\n",
              " 'Persian',\n",
              " 'Urdu',\n",
              " 'Portuguese',\n",
              " 'English',\n",
              " 'Latin',\n",
              " 'Hindi',\n",
              " 'Spanish',\n",
              " 'Chinese',\n",
              " 'French',\n",
              " 'Chinese',\n",
              " 'Latin',\n",
              " 'Arabic',\n",
              " 'Chinese',\n",
              " 'Indonesian',\n",
              " 'Tamil',\n",
              " 'Swedish',\n",
              " 'Thai',\n",
              " 'English',\n",
              " 'Japanese',\n",
              " 'Turkish',\n",
              " 'Latin',\n",
              " 'Latin',\n",
              " 'Korean',\n",
              " 'Romanian',\n",
              " 'Dutch',\n",
              " 'Dutch',\n",
              " 'Spanish',\n",
              " 'Romanian',\n",
              " 'Korean',\n",
              " 'Pushto',\n",
              " 'Chinese',\n",
              " 'English',\n",
              " 'Turkish',\n",
              " 'Urdu',\n",
              " 'English',\n",
              " 'Spanish',\n",
              " 'English',\n",
              " 'Portuguese',\n",
              " 'Romanian',\n",
              " 'Spanish',\n",
              " 'Indonesian',\n",
              " 'Swedish',\n",
              " 'Japanese',\n",
              " 'Thai',\n",
              " 'Dutch',\n",
              " 'Japanese',\n",
              " 'Swedish',\n",
              " 'Estonian',\n",
              " 'Arabic',\n",
              " 'Turkish',\n",
              " 'Swedish',\n",
              " 'Swedish',\n",
              " 'Russian',\n",
              " 'Hindi',\n",
              " 'Malay (macrolanguage)',\n",
              " 'French',\n",
              " 'Latin',\n",
              " 'Korean',\n",
              " 'Russian',\n",
              " 'Thai',\n",
              " 'Urdu',\n",
              " 'Estonian',\n",
              " 'Arabic',\n",
              " 'Persian',\n",
              " 'French',\n",
              " 'Persian',\n",
              " 'Pushto',\n",
              " 'English',\n",
              " 'Portuguese',\n",
              " 'Korean',\n",
              " 'Chinese',\n",
              " 'Indonesian',\n",
              " 'Portuguese',\n",
              " 'Romanian',\n",
              " 'Dutch',\n",
              " 'Swedish',\n",
              " 'Estonian',\n",
              " 'Japanese',\n",
              " 'Indonesian',\n",
              " 'Persian',\n",
              " 'Latin',\n",
              " 'Korean',\n",
              " 'Romanian',\n",
              " 'Japanese',\n",
              " 'French',\n",
              " 'Hindi',\n",
              " 'Russian',\n",
              " 'Persian',\n",
              " 'Turkish',\n",
              " 'Russian',\n",
              " 'French',\n",
              " 'Pushto',\n",
              " 'Chinese',\n",
              " 'Korean',\n",
              " 'Hindi',\n",
              " 'Arabic',\n",
              " 'Russian',\n",
              " 'Turkish',\n",
              " 'Korean',\n",
              " 'Indonesian',\n",
              " 'Pushto',\n",
              " 'Arabic',\n",
              " 'Latin',\n",
              " 'Hindi',\n",
              " 'Portuguese',\n",
              " 'Russian',\n",
              " 'Thai',\n",
              " 'Dutch',\n",
              " 'Tamil',\n",
              " 'Kirghiz',\n",
              " 'Thai',\n",
              " 'Portuguese',\n",
              " 'Persian',\n",
              " 'Romanian',\n",
              " 'Pushto',\n",
              " 'Persian',\n",
              " 'Hindi',\n",
              " 'Spanish',\n",
              " 'Tamil',\n",
              " 'Persian',\n",
              " 'Portuguese',\n",
              " 'Spanish',\n",
              " 'Dutch',\n",
              " 'English',\n",
              " 'Portuguese',\n",
              " 'Arabic',\n",
              " 'Turkish',\n",
              " 'English',\n",
              " 'Spanish',\n",
              " 'Japanese',\n",
              " 'Korean',\n",
              " 'Chinese',\n",
              " 'Latin',\n",
              " 'Estonian',\n",
              " 'Russian',\n",
              " 'Tamil',\n",
              " 'Spanish',\n",
              " 'Swedish',\n",
              " 'Latin',\n",
              " 'Japanese',\n",
              " 'Korean',\n",
              " 'Spanish',\n",
              " 'Estonian',\n",
              " 'Turkish',\n",
              " 'Russian',\n",
              " 'English',\n",
              " 'Korean',\n",
              " 'Latin',\n",
              " 'Estonian',\n",
              " 'Urdu',\n",
              " 'Portuguese',\n",
              " 'French',\n",
              " 'Swedish',\n",
              " 'Russian',\n",
              " 'Romanian',\n",
              " 'Dutch',\n",
              " 'Japanese',\n",
              " 'Hindi',\n",
              " 'Dutch',\n",
              " 'Portuguese',\n",
              " 'Dutch',\n",
              " 'Tamil',\n",
              " 'Portuguese',\n",
              " 'Romanian',\n",
              " 'Romanian',\n",
              " 'Portuguese',\n",
              " 'Arabic',\n",
              " 'Hindi',\n",
              " 'Turkish',\n",
              " 'Hindi',\n",
              " 'French',\n",
              " 'Japanese',\n",
              " 'Japanese',\n",
              " 'Russian',\n",
              " 'Arabic',\n",
              " 'Persian',\n",
              " 'Estonian',\n",
              " 'Tamil',\n",
              " 'Tamil',\n",
              " 'Estonian',\n",
              " 'Thai',\n",
              " 'Thai',\n",
              " 'Turkish',\n",
              " 'Tamil',\n",
              " 'Latin',\n",
              " 'Persian',\n",
              " 'French',\n",
              " 'Urdu',\n",
              " 'French',\n",
              " 'Dutch',\n",
              " 'Indonesian',\n",
              " 'Swedish',\n",
              " 'Hindi',\n",
              " 'Tamil',\n",
              " 'Chinese',\n",
              " 'Arabic',\n",
              " 'Kirghiz',\n",
              " 'Tamil',\n",
              " 'Japanese',\n",
              " 'French',\n",
              " 'Dutch',\n",
              " 'Chinese',\n",
              " 'Latin',\n",
              " 'Spanish',\n",
              " 'Thai',\n",
              " 'Japanese',\n",
              " 'Dutch',\n",
              " 'Portuguese',\n",
              " 'Turkish',\n",
              " 'Chinese',\n",
              " 'Russian',\n",
              " 'Tamil',\n",
              " 'Urdu',\n",
              " 'Swedish',\n",
              " 'Malay (macrolanguage)',\n",
              " 'Tamil',\n",
              " 'Estonian',\n",
              " 'Hindi',\n",
              " 'Arabic',\n",
              " 'Pushto',\n",
              " 'Russian',\n",
              " 'Japanese',\n",
              " 'Arabic',\n",
              " 'Thai',\n",
              " 'Dutch',\n",
              " 'Turkish',\n",
              " 'French',\n",
              " 'Persian',\n",
              " 'Arabic',\n",
              " 'Tamil',\n",
              " 'Tamil',\n",
              " 'Russian',\n",
              " 'French',\n",
              " 'Portuguese',\n",
              " 'Pushto',\n",
              " 'Thai',\n",
              " 'Japanese',\n",
              " 'Russian',\n",
              " 'French',\n",
              " 'Turkish',\n",
              " 'Russian',\n",
              " 'Portuguese',\n",
              " 'Urdu',\n",
              " 'Russian',\n",
              " 'Thai',\n",
              " 'Swedish',\n",
              " 'Thai',\n",
              " 'Swedish',\n",
              " 'Thai',\n",
              " 'Japanese',\n",
              " 'Luxembourgish',\n",
              " 'Latin',\n",
              " 'Korean',\n",
              " 'Russian',\n",
              " 'Russian',\n",
              " 'French',\n",
              " 'Thai',\n",
              " 'Portuguese',\n",
              " 'Indonesian',\n",
              " 'Dutch',\n",
              " 'Japanese',\n",
              " 'Tamil',\n",
              " 'Thai',\n",
              " 'English',\n",
              " 'Romanian',\n",
              " 'Japanese',\n",
              " 'Tamil',\n",
              " 'Arabic',\n",
              " 'Tamil',\n",
              " 'Swedish',\n",
              " 'Portuguese',\n",
              " 'Japanese',\n",
              " 'French',\n",
              " 'Indonesian',\n",
              " 'Swedish',\n",
              " 'Swedish',\n",
              " 'Thai',\n",
              " 'French',\n",
              " 'Tamil',\n",
              " 'Arabic',\n",
              " 'Romanian',\n",
              " 'Hindi',\n",
              " 'Indonesian',\n",
              " 'Chinese',\n",
              " 'Persian',\n",
              " 'Indonesian',\n",
              " 'Swedish',\n",
              " 'Portuguese',\n",
              " 'Korean',\n",
              " 'Hindi',\n",
              " 'Indonesian',\n",
              " 'Romanian',\n",
              " 'Korean',\n",
              " 'Tamil',\n",
              " 'English',\n",
              " 'Russian',\n",
              " 'Pushto',\n",
              " 'French',\n",
              " 'Korean',\n",
              " 'Urdu',\n",
              " 'Turkish',\n",
              " 'Portuguese',\n",
              " 'Indonesian',\n",
              " 'Spanish',\n",
              " 'Russian',\n",
              " 'Thai',\n",
              " 'Arabic',\n",
              " 'Estonian',\n",
              " 'Persian',\n",
              " 'French',\n",
              " 'Indonesian',\n",
              " 'Hindi',\n",
              " 'Japanese',\n",
              " 'Arabic',\n",
              " 'Thai',\n",
              " 'Korean',\n",
              " 'Swedish',\n",
              " 'Persian',\n",
              " 'Estonian',\n",
              " 'French',\n",
              " 'Latin',\n",
              " 'Japanese',\n",
              " 'Chinese',\n",
              " 'Portuguese',\n",
              " 'Estonian',\n",
              " 'Chinese',\n",
              " 'Russian',\n",
              " 'Chinese',\n",
              " 'Portuguese',\n",
              " 'Turkish',\n",
              " 'Persian',\n",
              " 'Persian',\n",
              " 'Russian',\n",
              " 'Pushto',\n",
              " 'Thai',\n",
              " 'Latin',\n",
              " 'Dutch',\n",
              " 'Romanian',\n",
              " 'Japanese',\n",
              " 'Tamil',\n",
              " 'Persian',\n",
              " 'Korean',\n",
              " 'Urdu',\n",
              " 'Latin',\n",
              " 'Dutch',\n",
              " 'Turkish',\n",
              " 'Dutch',\n",
              " 'Tamil',\n",
              " 'Persian',\n",
              " 'Estonian',\n",
              " 'English',\n",
              " 'Arabic',\n",
              " 'Chinese',\n",
              " 'Romanian',\n",
              " 'Hindi',\n",
              " 'Russian',\n",
              " 'Japanese',\n",
              " 'Russian',\n",
              " 'Hindi',\n",
              " 'Romanian',\n",
              " 'Thai',\n",
              " 'Japanese',\n",
              " 'Swedish',\n",
              " 'Hindi',\n",
              " 'Urdu',\n",
              " 'Turkish',\n",
              " 'Persian',\n",
              " 'French',\n",
              " 'English',\n",
              " 'Hindi',\n",
              " 'Latin',\n",
              " 'Romanian',\n",
              " 'Japanese',\n",
              " 'Latin',\n",
              " 'English',\n",
              " 'Dutch',\n",
              " 'Hindi',\n",
              " 'Turkish',\n",
              " 'Hindi',\n",
              " 'Portuguese',\n",
              " 'Dutch',\n",
              " 'Tamil',\n",
              " 'French',\n",
              " 'Estonian',\n",
              " 'Pushto',\n",
              " 'French',\n",
              " 'Dutch',\n",
              " 'Swedish',\n",
              " 'Urdu',\n",
              " 'Romanian',\n",
              " 'Russian',\n",
              " 'Persian',\n",
              " 'Tamil',\n",
              " 'Swedish',\n",
              " 'Latin',\n",
              " 'Estonian',\n",
              " 'Swedish',\n",
              " 'Spanish',\n",
              " 'Swedish',\n",
              " 'Spanish',\n",
              " 'Spanish',\n",
              " 'Estonian',\n",
              " 'Latin',\n",
              " 'Turkish',\n",
              " 'Russian',\n",
              " 'Thai',\n",
              " 'Indonesian',\n",
              " 'Korean',\n",
              " 'Pushto',\n",
              " 'Portuguese',\n",
              " 'Tamil',\n",
              " 'French',\n",
              " 'Pushto',\n",
              " 'Urdu',\n",
              " 'Arabic',\n",
              " 'Spanish',\n",
              " 'Tamil',\n",
              " 'Latin',\n",
              " 'Spanish',\n",
              " 'Russian',\n",
              " 'Turkish',\n",
              " 'Japanese',\n",
              " 'Turkish',\n",
              " 'Turkish',\n",
              " 'Chinese',\n",
              " 'Japanese',\n",
              " 'Arabic',\n",
              " 'French',\n",
              " 'Romanian',\n",
              " 'Korean',\n",
              " 'French',\n",
              " 'Pushto',\n",
              " 'French',\n",
              " 'Japanese',\n",
              " 'Persian',\n",
              " 'Turkish',\n",
              " 'Indonesian',\n",
              " 'Dutch',\n",
              " 'Indonesian',\n",
              " 'Swedish',\n",
              " 'Russian',\n",
              " 'Romanian',\n",
              " 'Indonesian',\n",
              " 'Tamil',\n",
              " 'Estonian',\n",
              " 'Turkish',\n",
              " 'English',\n",
              " 'Russian',\n",
              " 'Thai',\n",
              " 'Romanian',\n",
              " 'Russian',\n",
              " 'Portuguese',\n",
              " 'Arabic',\n",
              " 'Russian',\n",
              " 'Arabic',\n",
              " 'Latin',\n",
              " 'Korean',\n",
              " 'Tamil',\n",
              " 'Persian',\n",
              " 'Chinese',\n",
              " 'Persian',\n",
              " 'Estonian',\n",
              " 'Tamil',\n",
              " 'Japanese',\n",
              " 'Arabic',\n",
              " 'Turkish',\n",
              " 'Estonian',\n",
              " 'Indonesian',\n",
              " 'Swedish',\n",
              " 'Romanian',\n",
              " 'Swedish',\n",
              " 'Thai',\n",
              " 'Chinese',\n",
              " 'Indonesian',\n",
              " 'Hindi',\n",
              " 'Spanish',\n",
              " 'Urdu',\n",
              " 'Portuguese',\n",
              " 'English',\n",
              " 'Swedish',\n",
              " 'Russian',\n",
              " 'Pushto',\n",
              " 'Spanish',\n",
              " 'Tamil',\n",
              " 'Dutch',\n",
              " 'Russian',\n",
              " 'Indonesian',\n",
              " 'Japanese',\n",
              " 'Dutch',\n",
              " 'Spanish',\n",
              " 'Latin',\n",
              " 'Japanese',\n",
              " 'Estonian',\n",
              " 'Dutch',\n",
              " 'Japanese',\n",
              " 'English',\n",
              " 'Urdu',\n",
              " 'Spanish',\n",
              " 'Chinese',\n",
              " 'Korean',\n",
              " 'Japanese',\n",
              " 'Chinese',\n",
              " 'Persian',\n",
              " 'Tamil',\n",
              " 'Chinese',\n",
              " 'Thai',\n",
              " 'Dutch',\n",
              " 'Russian',\n",
              " 'Turkish',\n",
              " 'Persian',\n",
              " 'Arabic',\n",
              " 'Malay (macrolanguage)',\n",
              " 'Persian',\n",
              " 'Tamil',\n",
              " 'Arabic',\n",
              " 'Arabic',\n",
              " 'Spanish',\n",
              " 'Thai',\n",
              " 'Russian',\n",
              " 'Portuguese',\n",
              " 'Chinese',\n",
              " 'Urdu',\n",
              " 'Estonian',\n",
              " 'Persian',\n",
              " 'Japanese',\n",
              " 'Russian',\n",
              " 'Dutch',\n",
              " 'Latin',\n",
              " 'Tamil',\n",
              " 'Urdu',\n",
              " 'Dutch',\n",
              " 'Arabic',\n",
              " 'Persian',\n",
              " 'Russian',\n",
              " 'Turkish',\n",
              " 'Turkish',\n",
              " 'Estonian',\n",
              " 'Thai',\n",
              " 'German',\n",
              " 'English',\n",
              " 'Urdu',\n",
              " 'Russian',\n",
              " 'Tamil',\n",
              " 'Chinese',\n",
              " 'English',\n",
              " 'Latin',\n",
              " 'Spanish',\n",
              " 'Latin',\n",
              " 'Dutch',\n",
              " 'Korean',\n",
              " 'Korean',\n",
              " 'Estonian',\n",
              " 'Chinese',\n",
              " 'French',\n",
              " 'Japanese',\n",
              " 'Latin',\n",
              " 'Latin',\n",
              " 'Indonesian',\n",
              " 'Persian',\n",
              " 'Persian',\n",
              " 'Persian',\n",
              " 'Swedish',\n",
              " 'English',\n",
              " 'Romanian',\n",
              " 'Russian',\n",
              " 'Korean',\n",
              " 'Indonesian',\n",
              " 'Pushto',\n",
              " 'Japanese',\n",
              " 'Tamil',\n",
              " 'Thai',\n",
              " 'Japanese',\n",
              " 'Pushto',\n",
              " 'Turkish',\n",
              " 'Tamil',\n",
              " 'French',\n",
              " 'Persian',\n",
              " 'French',\n",
              " 'Thai',\n",
              " 'English',\n",
              " 'Urdu',\n",
              " 'Pushto',\n",
              " 'Tamil',\n",
              " 'Persian',\n",
              " 'Thai',\n",
              " 'Arabic',\n",
              " 'Indonesian',\n",
              " 'Japanese',\n",
              " 'Persian',\n",
              " 'Portuguese',\n",
              " 'Urdu',\n",
              " 'Estonian',\n",
              " 'Arabic',\n",
              " 'English',\n",
              " 'Japanese',\n",
              " 'Russian',\n",
              " 'Hindi',\n",
              " 'Korean',\n",
              " 'Persian',\n",
              " 'Hindi',\n",
              " 'Romanian',\n",
              " 'Korean',\n",
              " 'Japanese',\n",
              " 'Thai',\n",
              " 'Dutch',\n",
              " 'Chinese',\n",
              " 'Japanese',\n",
              " 'Thai',\n",
              " 'Chinese',\n",
              " 'Portuguese',\n",
              " 'Indonesian',\n",
              " 'Russian',\n",
              " 'Pushto',\n",
              " 'Pushto',\n",
              " 'Urdu',\n",
              " 'Romanian',\n",
              " 'Hindi',\n",
              " 'Portuguese',\n",
              " 'Spanish',\n",
              " 'Turkish',\n",
              " 'Portuguese',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dLGBtL1wFjk",
        "outputId": "ae8c31ef-d775-4406-d313-8237f5475c4a"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(\"FastText\")\n",
        "print(accuracy_score(res1, y))\n",
        "print(\"---------\")\n",
        "print(\"LangId\")\n",
        "print(accuracy_score(res2, y))\n",
        "#print(accuracy_score(results_langdetect, y)\n",
        "print(\"---------\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText\n",
            "0.9677272727272728\n",
            "---------\n",
            "LangId\n",
            "0.9542727272727273\n",
            "---------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        Estonian\n",
              "1         Swedish\n",
              "2            Thai\n",
              "3           Tamil\n",
              "4           Dutch\n",
              "           ...   \n",
              "21995      French\n",
              "21996        Thai\n",
              "21997     Spanish\n",
              "21998     Chinese\n",
              "21999    Romanian\n",
              "Name: language, Length: 22000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIOgOZy1ENh2"
      },
      "source": [
        "# Exercise 2\n",
        "\n",
        "For English-written text, apply word-level tokenization. What is the average number of words per sentence?\n",
        "\n",
        "Implement word-tokenization using both [nltk](https://www.nltk.org/) and [spacy](https://spacy.io/). Report the results for both of them.\n",
        "\n",
        "For spaCy use the `en_core_web_sm` model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "R6eoMaao_rVp",
        "outputId": "b3508d81-f280-49f6-b474-063b8e52a61b"
      },
      "source": [
        "df_eng = df.drop(df[df.language !=\"English\"].index)\n",
        "df_eng.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>in  johnson was awarded an american institute ...</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>bussy-saint-georges has built its identity on ...</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>minnesotas state parks are spread across the s...</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>nordahl road is a station served by north coun...</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>a talk by takis fotopoulos about the internati...</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Text language\n",
              "37  in  johnson was awarded an american institute ...  English\n",
              "40  bussy-saint-georges has built its identity on ...  English\n",
              "76  minnesotas state parks are spread across the s...  English\n",
              "90  nordahl road is a station served by north coun...  English\n",
              "97  a talk by takis fotopoulos about the internati...  English"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTGXzYn4Be46"
      },
      "source": [
        "# Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ph9mCx00sQ"
      },
      "source": [
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "token_list = []\n",
        "for text in df_eng['Text']:\n",
        "    \n",
        "    my_doc = nlp(text)\n",
        "\n",
        "    # Create list of word tokens\n",
        "    \n",
        "    for token in my_doc:\n",
        "        token_list.append(token.text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbMtS3zaBh6S"
      },
      "source": [
        "# NLTK "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20n6gl6RBrD9",
        "outputId": "2544484a-df8b-42e2-9575-acb979f0ceea"
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "import numpy as np\n",
        "tk = []\n",
        "for text in df_eng['Text']:\n",
        "    \n",
        "    tokens = word_tokenize(text)\n",
        "    tk = tk + tokens\n",
        "    \n",
        "len(tk)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68738"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceajr4ATCUxu"
      },
      "source": [
        "t1 = len(token_list)\n",
        "t2 = len(tk)\n",
        "d = len(df_eng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghO1CvNyMjP4",
        "outputId": "d26246ef-af2a-45a5-cdca-b5207b71498b"
      },
      "source": [
        "print(\"NLTK: \",t1/d )\n",
        "print(\"SpaCy: \",t2/d )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK:  72.334\n",
            "SpaCy:  68.738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySnw80QnIf2p"
      },
      "source": [
        "# Exercise 3\n",
        "\n",
        "Use spacy to parse the dependency tree of a **randomly selected** sentence. You can both use English sentences or your native language (if supported in [spaCy](https://spacy.io/usage/models/)). Use [displaCy](https://explosion.ai/demos/displacy) to visualize the result in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoFgoxwJ03_c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "8b3bd58a-8a63-4ac9-b997-2f8577a4817e"
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(\"Hi, my name is Gianluca and my surname is LM\")\n",
        "displacy.render(doc, style=\"dep\",jupyter=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"bfbd2c0ca29645e28c841d08395b8745-0\" class=\"displacy\" width=\"1800\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Hi,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">INTJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">my</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">name</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">Gianluca</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">and</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">CCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">my</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">surname</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">LM</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bfbd2c0ca29645e28c841d08395b8745-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bfbd2c0ca29645e28c841d08395b8745-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">intj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bfbd2c0ca29645e28c841d08395b8745-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bfbd2c0ca29645e28c841d08395b8745-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bfbd2c0ca29645e28c841d08395b8745-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bfbd2c0ca29645e28c841d08395b8745-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bfbd2c0ca29645e28c841d08395b8745-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bfbd2c0ca29645e28c841d08395b8745-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bfbd2c0ca29645e28c841d08395b8745-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bfbd2c0ca29645e28c841d08395b8745-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bfbd2c0ca29645e28c841d08395b8745-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bfbd2c0ca29645e28c841d08395b8745-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bfbd2c0ca29645e28c841d08395b8745-0-6\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1270.0,89.5 1270.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bfbd2c0ca29645e28c841d08395b8745-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1270.0,266.5 L1278.0,254.5 1262.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bfbd2c0ca29645e28c841d08395b8745-0-7\" stroke-width=\"2px\" d=\"M595,264.5 C595,2.0 1450.0,2.0 1450.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bfbd2c0ca29645e28c841d08395b8745-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1450.0,266.5 L1458.0,254.5 1442.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-bfbd2c0ca29645e28c841d08395b8745-0-8\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,177.0 1615.0,177.0 1615.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-bfbd2c0ca29645e28c841d08395b8745-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1615.0,266.5 L1623.0,254.5 1607.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAO1E6adVMKW"
      },
      "source": [
        "# Exercise 4\n",
        "For the same sentence selected in the previous step apply all the following steps:\n",
        "1. Lemmatization: convert each word to its root form.\n",
        "2. Stopword removal: remove language-specific stopwords.\n",
        "3. Part of Speech Tagging: for each word in the sentence display its part-of-speech.\n",
        "\n",
        "For each step, print the resulting list on the console."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4-wJHnOZnwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "320bd872-ada2-4fd4-eb84-18ea2eefad72"
      },
      "source": [
        "#Lemmatization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(\"Hi, my name is Gianluca and my surname is LM\")\n",
        "lemmas = \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "#Stopwords removal\n",
        "all_stopwords = nlp.Defaults.stop_words\n",
        "tokens = word_tokenize(lemmas)\n",
        "tokens_without_sw = [word for word in tokens if not word in all_stopwords]\n",
        "\n",
        "\n",
        "#Part of speech tagging\n",
        "for word in doc:\n",
        "    print(word,\": \",word.pos_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi :  INTJ\n",
            ", :  PUNCT\n",
            "my :  PRON\n",
            "name :  NOUN\n",
            "is :  AUX\n",
            "Gianluca :  PROPN\n",
            "and :  CCONJ\n",
            "my :  PRON\n",
            "surname :  NOUN\n",
            "is :  VERB\n",
            "LM :  PROPN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL4KXiBkZrta"
      },
      "source": [
        "# **Occurrence-based text representation - TF-IDF**\n",
        "\n",
        "---\n",
        "TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. It allows to create occurrence-based vector representation for each document.\n",
        "\n",
        "# Exercise 5\n",
        "Use TF-IDF to vectorize each sentence in the original data collection. You can choose your preferred implementation for TF-IDF vectorization. It is also available on [SciKit-Learn library](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7L162ayglUq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ce583e-29ba-4df9-ad44-cd217770f69b"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['Text'])\n",
        "\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 43365)\t0.15245962403688543\n",
            "  (0, 122097)\t0.15245962403688543\n",
            "  (0, 76696)\t0.13890427169403846\n",
            "  (0, 45787)\t0.15245962403688543\n",
            "  (0, 113244)\t0.13890427169403846\n",
            "  (0, 43364)\t0.15245962403688543\n",
            "  (0, 75247)\t0.22902894148770514\n",
            "  (0, 153)\t0.2155525828166711\n",
            "  (0, 55264)\t0.2678531884721598\n",
            "  (0, 63450)\t0.24339387890154524\n",
            "  (0, 122096)\t0.15245962403688543\n",
            "  (0, 59244)\t0.15245962403688543\n",
            "  (0, 122428)\t0.11632821567894924\n",
            "  (0, 67654)\t0.15245962403688543\n",
            "  (0, 106284)\t0.0828549222249433\n",
            "  (0, 117123)\t0.1339265942360799\n",
            "  (0, 136)\t0.08509082541842236\n",
            "  (0, 112023)\t0.15245962403688543\n",
            "  (0, 60954)\t0.14646128506875586\n",
            "  (0, 49445)\t0.15245962403688543\n",
            "  (0, 45293)\t0.12265170453053793\n",
            "  (0, 80288)\t0.15245962403688543\n",
            "  (0, 79323)\t0.15245962403688543\n",
            "  (0, 53103)\t0.13228208686853668\n",
            "  (0, 47020)\t0.14646128506875586\n",
            "  :\t:\n",
            "  (21999, 102253)\t0.18987120980426156\n",
            "  (21999, 101536)\t0.19555363016241606\n",
            "  (21999, 69301)\t0.3911072603248321\n",
            "  (21999, 95538)\t0.18546358214789696\n",
            "  (21999, 69551)\t0.18987120980426156\n",
            "  (21999, 101741)\t0.3576348748525535\n",
            "  (21999, 70726)\t0.18987120980426156\n",
            "  (21999, 84356)\t0.17385336645935637\n",
            "  (21999, 103844)\t0.18186228813180896\n",
            "  (21999, 6037)\t0.16016202442874922\n",
            "  (21999, 40786)\t0.15215310275629662\n",
            "  (21999, 104843)\t0.16248852574304734\n",
            "  (21999, 6023)\t0.14759970003791315\n",
            "  (21999, 38077)\t0.1309466755562267\n",
            "  (21999, 123333)\t0.16988963234321575\n",
            "  (21999, 81608)\t0.10343937006427364\n",
            "  (21999, 66036)\t0.10270771489197011\n",
            "  (21999, 74014)\t0.13510584168183312\n",
            "  (21999, 4888)\t0.1290413507799354\n",
            "  (21999, 17371)\t0.20944489288925866\n",
            "  (21999, 123302)\t0.08625573470313737\n",
            "  (21999, 124865)\t0.0886604635701815\n",
            "  (21999, 28194)\t0.09160270401248888\n",
            "  (21999, 97733)\t0.07526526548636828\n",
            "  (21999, 25042)\t0.19212934088982778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4NTwm6Qbpvx"
      },
      "source": [
        "# Exercise 6\n",
        "\n",
        "Build a supervised multi-class language detector using as features the vector obtained by TF-IDF representation. Use 80% of the data to train the language detector and 20% of the data for assessing its accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1HT2roy1DQu"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8)\n",
        "clf = LogisticRegression().fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYNPh8JKgX_e",
        "outputId": "2b059bc7-ed2e-42ac-f58a-77b0a4bccf6b"
      },
      "source": [
        "accuracy_score(y_pred,y_test) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9286931818181818"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AHlGUHuaqjW"
      },
      "source": [
        "# **Topic Modelling**\n",
        "\n",
        "Occurrence-based representations are high-dimensional, what is the dimension of the generated TF-IDF vector representation?\n",
        "Topic modelling focuses on caturing latent topics in large document corpora.\n",
        "\n",
        "The data collection used in this second part of the practice is provided [here](https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P1/CovidFake_filtered.csv) - [source: Zenodo](https://zenodo.org/record/4282522#.YVdCXcbOOpd)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZbbPhC6cAO8"
      },
      "source": [
        "# Exercise 7\n",
        "\n",
        "Latent Semantic Indexing (LSI) models underlying concepts by using SVD (Singular Value Decomposition).\n",
        "\n",
        "Use [gensim](https://radimrehurek.com/gensim/) library to:\n",
        "1. Create a corpus composed of the headlines contained in the data collection.\n",
        "2. Generate a [dictionary](https://radimrehurek.com/gensim/corpora/dictionary.html) to create a word -> id mapping (required by LSI module).\n",
        "3. Using the dictionary, preprocess the corpus to obtain the representation required for LSI model training ([documentation here](https://radimrehurek.com/gensim/models/lsimodel.html)).\n",
        "4. Inspect the top-5 topics generated by the LSI model for the analysed corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwkHsT8oft_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdf3c25c-d431-4182-a161-7c0bfdb0c8a7"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P1/CovidFake_filtered.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-07 09:46:01--  https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P1/CovidFake_filtered.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1088708 (1.0M) [text/plain]\n",
            "Saving to: ‘CovidFake_filtered.csv’\n",
            "\n",
            "\rCovidFake_filtered.   0%[                    ]       0  --.-KB/s               \rCovidFake_filtered. 100%[===================>]   1.04M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-10-07 09:46:01 (104 MB/s) - ‘CovidFake_filtered.csv’ saved [1088708/1088708]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoJAZsw11Gih",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d0ddfd7f-6c0b-42a8-839d-ca7f70807580"
      },
      "source": [
        "import pandas as pd\n",
        "from gensim.test.utils import common_dictionary, common_corpus\n",
        "from gensim.models import LsiModel\n",
        "df_c = pd.read_csv(\"CovidFake_filtered.csv\")\n",
        "df_c.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>headlines</th>\n",
              "      <th>outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A post claims compulsory vacination violates t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>A photo claims that this person is a doctor wh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Post about a video claims that it is a protest...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>All deaths by respiratory failure and pneumoni...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>The dean of the College of Biologists of Euska...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                          headlines  outcome\n",
              "0           0  A post claims compulsory vacination violates t...        0\n",
              "1           1  A photo claims that this person is a doctor wh...        0\n",
              "2           2  Post about a video claims that it is a protest...        0\n",
              "3           3  All deaths by respiratory failure and pneumoni...        0\n",
              "4           4  The dean of the College of Biologists of Euska...        0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIh6-sYFJmR-",
        "outputId": "d00d2b6e-9cd8-419d-b737-9885c5b6657a"
      },
      "source": [
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.test.utils import common_dictionary, common_corpus\n",
        "from gensim.models import LsiModel\n",
        "\n",
        "corpus = df_c['headlines'].tolist()\n",
        "corpus = [s.split() for s in corpus]\n",
        "\n",
        "c_dict = Dictionary(corpus) \n",
        "processed_corpus = [c_dict.doc2bow(text) for text in corpus]\n",
        "\n",
        "\n",
        "\n",
        "model = LsiModel(processed_corpus, id2word=c_dict)\n",
        "model.print_topics(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.636*\"the\" + 0.389*\"of\" + 0.314*\"in\" + 0.280*\"a\" + 0.254*\"to\" + 0.179*\"and\" + 0.155*\"that\" + 0.121*\"is\" + 0.107*\"coronavirus\" + 0.106*\"A\"'),\n",
              " (1,\n",
              "  '0.629*\"the\" + -0.598*\"a\" + -0.386*\"in\" + -0.105*\"A\" + -0.097*\"and\" + -0.078*\"has\" + -0.077*\"COVID-19\" + -0.071*\"video\" + -0.070*\"on\" + -0.064*\"been\"'),\n",
              " (2,\n",
              "  '0.709*\"to\" + -0.633*\"of\" + 0.137*\"and\" + 0.123*\"is\" + 0.071*\"for\" + 0.069*\"be\" + 0.054*\"that\" + 0.051*\"are\" + 0.049*\"from\" + 0.048*\"due\"'),\n",
              " (3,\n",
              "  '-0.734*\"in\" + 0.428*\"of\" + 0.363*\"to\" + 0.283*\"a\" + -0.172*\"the\" + -0.097*\"coronavirus\" + 0.046*\"from\" + -0.040*\"was\" + 0.037*\"COVID-19.\" + 0.034*\"on\"'),\n",
              " (4,\n",
              "  '0.479*\"to\" + 0.425*\"of\" + -0.407*\"a\" + 0.391*\"in\" + -0.259*\"that\" + -0.240*\"the\" + -0.213*\"is\" + -0.187*\"and\" + -0.115*\"for\" + -0.075*\"coronavirus\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zcmfh5UkdZm"
      },
      "source": [
        "# Exercise 8 (Optional)\n",
        "\n",
        "The top-scored words contributing to each topic (if no stopword removal is applied) are english common words (e.g., *to, for, in, of, on*..). Repeat the same procedure of Ex. 7 by adding a preliminary preprocessing step to **remove stopwords**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz2NUP6O1JAK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1f51257-1b52-45cd-9f2d-09ab4450c815"
      },
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "rs_corpus = [[w.lower() for w in s if w.lower() not in stop_words] for s in corpus]\n",
        "rs_corpus = [[w.translate(str.maketrans('', '', string.punctuation)) for w in s] for s in rs_corpus]\n",
        "print(rs_corpus[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[['post', 'claims', 'compulsory', 'vacination', 'violates', 'principles', 'bioethics', 'coronavirus', 'exist', 'pcr', 'test', 'returns', 'many', 'false', 'positives', 'influenza', 'vaccine', 'related', 'covid19'], ['photo', 'claims', 'person', 'doctor', 'died', 'attending', 'many', 'covid19', 'patinents', 'hospital', 'muñiz', 'buenos', 'aires'], ['post', 'video', 'claims', 'protest', 'confination', 'town', 'aranda', 'de', 'duero', 'burgos'], ['deaths', 'respiratory', 'failure', 'pneumonia', 'registered', 'covid19', 'according', 'civil', 'registry', 'website'], ['dean', 'college', 'biologists', 'euskadi', 'states', 'lot', 'pcr', 'false', 'positives', 'asymptomatic', 'spread', 'coronavirus'], ['households', 'covid19', 'patients', 'porto', 'alegre', 'campo', 'grande', 'santo', 'antônio', 'da', 'platina', 'must', 'put', 'red', 'ribbon', 'garbage', 'bags', 'garbagemen', 'instructed', 'handle', 'safer', 'way'], ['chain', 'lists', 'recommendations', 'prevent', 'treat', 'coronavirus'], ['60000', 'argentinian', 'companies', 'closed', 'due', 'covid19', 'also', 'companies', 'receiving', 'government', 'aid', 'pandemic', 'expropriated', 'government'], ['social', 'media', 'posts', 'criticize', 'photo', 'smiling', 'president', 'goverment', 'pedro', 'sánchez', 'holidays', 'thousands', 'died', 'covid19'], ['cdc', 'released', 'update', 'novel', 'coronavirus', 'transmitted']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tmkg7zyhO4aR",
        "outputId": "0c13a52c-d7bd-47f4-9a16-fb0ff52d72c0"
      },
      "source": [
        "rs_tm_dict = Dictionary(corpus)\n",
        "rs_processed_corpus = [rs_tm_dict.doc2bow(text) for text in rs_corpus]\n",
        "rs_model = LsiModel(rs_processed_corpus, id2word=rs_tm_dict)\n",
        "rs_model.print_topics(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.743*\"coronavirus\" + 0.406*\"covid19\" + 0.172*\"video\" + 0.139*\"people\" + 0.120*\"shows\" + 0.120*\"facebook\" + 0.109*\"novel\" + 0.107*\"claim\" + 0.098*\"new\" + 0.096*\"shared\"'),\n",
              " (1,\n",
              "  '0.812*\"covid19\" + -0.544*\"coronavirus\" + 0.065*\"video\" + 0.050*\"shows\" + -0.045*\"novel\" + -0.040*\"new\" + 0.039*\"hospital\" + 0.037*\"claims\" + 0.037*\"facebook\" + 0.034*\"lockdown\"'),\n",
              " (2,\n",
              "  '-0.358*\"video\" + -0.326*\"facebook\" + -0.297*\"claim\" + 0.294*\"covid19\" + -0.279*\"shows\" + -0.278*\"posts\" + -0.267*\"times\" + 0.250*\"coronavirus\" + -0.250*\"shared\" + -0.196*\"multiple\"'),\n",
              " (3,\n",
              "  '0.653*\"video\" + 0.300*\"shows\" + 0.253*\"people\" + -0.240*\"facebook\" + -0.237*\"posts\" + -0.214*\"claim\" + -0.202*\"shared\" + -0.175*\"times\" + -0.152*\"multiple\" + 0.143*\"lockdown\"'),\n",
              " (4,\n",
              "  '0.881*\"people\" + -0.303*\"video\" + -0.130*\"shows\" + -0.097*\"coronavirus\" + -0.089*\"covid19\" + 0.073*\"lockdown\" + 0.072*\"government\" + 0.068*\"virus\" + 0.065*\"died\" + -0.065*\"patients\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zssKPqSdyYgY"
      },
      "source": [
        "# Exercise 9 (Optional)\n",
        "\n",
        "Leveraging the same corpus used for LSI model generation, apply LDA modelling setting the number of topics to 5. Display the words most contributing to the those topics according to the LDA model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzCXdEUW1MHs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cca5c0e2-6d31-4506-df96-b459799bb583"
      },
      "source": [
        "from gensim.models.ldamodel import LdaModel\n",
        "lda = LdaModel(rs_processed_corpus, id2word=rs_tm_dict, num_topics=3)\n",
        "lda.print_topics(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.035*\"coronavirus\" + 0.027*\"covid19\" + 0.007*\"video\" + 0.005*\"president\" + 0.005*\"china\" + 0.005*\"outbreak\" + 0.005*\"patients\" + 0.004*\"government\" + 0.004*\"people\" + 0.004*\"new\"'),\n",
              " (1,\n",
              "  '0.032*\"coronavirus\" + 0.018*\"video\" + 0.015*\"shows\" + 0.014*\"covid19\" + 0.011*\"china\" + 0.010*\"people\" + 0.008*\"facebook\" + 0.007*\"claim\" + 0.006*\"novel\" + 0.006*\"shared\"'),\n",
              " (2,\n",
              "  '0.061*\"coronavirus\" + 0.023*\"covid19\" + 0.013*\"people\" + 0.011*\"new\" + 0.008*\"water\" + 0.007*\"virus\" + 0.007*\"wuhan\" + 0.006*\"novel\" + 0.006*\"cure\" + 0.006*\"vaccine\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBhdPyiKDNZ0"
      },
      "source": [
        "# Exercise 10 (Optional)\n",
        "\n",
        "Using [pyLDAvis]() library build an interactive visualization for the trained LDA model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aM6yrdPDREK"
      },
      "source": [
        "#import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "lda_display = gensimvis.prepare(lda, rs_processed_corpus, rs_tm_dict, sort_topics=False,jupyter=True)\n",
        "pyLDAvis.display(lda_display)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}